{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LMQL with local models\n",
    "\n",
    "To use all of LMQL's features, we need to host the models ourselves. LMQL supports\n",
    "the Huggingface Transformers API and the Llama.cpp API. (It appears that the work\n",
    "necessary to support Ollama is in progress, so hopefully soon!)\n",
    "\n",
    "In the meantime, we will need to use `llama-cpp-python` and download the model weights ourselves. We will use the 4-bit quantized version that Ollama uses by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the model to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'llama2/llama-2-7b.Q4_K_M.gguf' already exists.\n"
     ]
    }
   ],
   "source": [
    "import lmql_demo.download as download\n",
    "\n",
    "llama2_tokenizer = \"llama2/llama-tokenizer.model\"\n",
    "mistral_tokenizer = \"llama2/mistral-tokenizer.model\"\n",
    "model_filename = download.download_llama2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmql\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = lmql.model(f\"local:llama.cpp:{model_filename}\", tokenizer=llama2_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Joke:\n",
    "    setup: str\n",
    "    punchline: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lmql.query\n",
    "async def tell_joke():\n",
    "    '''lmql\n",
    "    sample(n=1, temperature=0.5)\n",
    "        \"\"\"This is a list of the funniest AI jokes ever told with a setup and a punchline. Q: indicates the setup and A: indicates the punchline\n",
    "        Q: Why was the AI bad at soccer?\n",
    "        A: Because it kept trying to reboot the ball.\n",
    "        \n",
    "        Q: Why did the AI cross the road?\n",
    "        A: To optimize the objective function.\n",
    "\n",
    "        \"\"\"\n",
    "        \"Q: [setup]\\n\" where len(TOKENS(setup)) < 120 and STOPS_AT(setup, \"?\") and STOPS_AT(setup, \".\")\n",
    "        \"A: [punchline]\\n\" where STOPS_AT(punchline, \"\\n\") and len(TOKENS(punchline)) < 120\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## AI Joke\n",
       "\n",
       "**Q:** 3 AI robots walk into a bar.\n",
       "\n",
       "**A:** 1: Hey, letâ€™s go get a drink.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmql_response = await tell_joke(model=local_model)\n",
    "joke = Joke(**lmql_response.variables)\n",
    "Markdown(f\"## AI Joke\\n\\n**Q:** {joke.setup}\\n\\n**A:** {joke.punchline}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmql-demo-g2XuB1Kg-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
